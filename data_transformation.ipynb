{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import isnull, count, log10, col, when, lit\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session and Context objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('ckpt2_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data into Spark Session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_account_info.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_Churn.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_demographics.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_services.csv']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_dir_path = os.path.abspath('Output_telecomData')\n",
    "subdirectory_name = 'WithNaNs'\n",
    "\n",
    "data_dir_path = os.path.join(main_data_dir_path, subdirectory_name)\n",
    "assert os.path.exists(data_dir_path)\n",
    "\n",
    "datafile_names = os.listdir(data_dir_path)\n",
    "datafile_paths = [os.path.join(data_dir_path, datafile) for datafile in datafile_names]\n",
    "\n",
    "datafile_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = spark.read.load(datafile_paths[0], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_churn_df = spark.read.load(datafile_paths[1], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_demo_df = spark.read.load(datafile_paths[2], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_serv_df = spark.read.load(datafile_paths[3], format='csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_acc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_churn_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- SeniorCitizen: double (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_serv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34413"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_churn_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df.createOrReplaceTempView('acc_df')\n",
    "cust_demo_df.createOrReplaceTempView('demo_df')\n",
    "cust_serv_df.createOrReplaceTempView('serv_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_count(df, col_name):\n",
    "    return df.filter(df[col_name]==np.nan).count()\n",
    "\n",
    "def replace_nan_in_col(df, col_name, by_value=0):\n",
    "    dtypes_dict = dict(df.dtypes)\n",
    "    col_dtype = dtypes_dict[col_name]\n",
    "    if col_dtype!='string':\n",
    "        return df.replace(np.nan, by_value, col_name)\n",
    "    return df.replace('NaN', by_value, col_name)\n",
    "\n",
    "def get_col_mean(df, col_name):\n",
    "    temp = df.replace(np.nan, 0)\n",
    "    return temp.agg({col_name:'avg'}).collect()[0][0]    \n",
    "\n",
    "def get_col_mode(df, col_name):\n",
    "    temp=df.replace(np.nan, 0)\n",
    "    return df.groupBy(col_name).count().orderBy(F.desc(\"count\")).first()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Tenure and Monthly Charges features:\n",
    "As shown in **task1.1** tenure and monthly charges both show normal-like distribution with piled up values at their respective highs and lows. \n",
    "\n",
    "<h4> Basic Treatment Strategy </h4> Mean imputation will work fine as the data is mostly normal like.<br>\n",
    "<h4> Complex Treatment Strategy </h4> Can categorize the data in three parts, the lows, the highs and rest. We can use models like random forest to classify the NaNs of both of the features into these three categories, and finally after classification we can impute the the highs and lows with their respective values and the rests with mean. (<b>But I feel it's a bit overkill as NaN count is not that high</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in Tenure: 16\n",
      "No. of NaN values in MonthlyCharges: 13\n"
     ]
    }
   ],
   "source": [
    "nan_count_tenure = get_nan_count(cust_acc_df, 'Tenure')\n",
    "nan_count_monthlyc = get_nan_count(cust_acc_df, 'MonthlyCharges')\n",
    "\n",
    "print(\"No. of NaN values in Tenure:\", nan_count_tenure)\n",
    "print(\"No. of NaN values in MonthlyCharges:\", nan_count_monthlyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tenure = get_col_mean(cust_acc_df, 'Tenure')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'Tenure', mean_tenure)\n",
    "\n",
    "mean_monthlyc = get_col_mean(cust_acc_df, 'MonthlyCharges')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'MonthlyCharges', mean_monthlyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Total Charges feature:\n",
    "As TotalCharges have a exponentially decreasing distribution, we can perform log transform on it to normalize the distribution.\n",
    "\n",
    "<h4> Imputation Strategy: </h4> Replace with mean of transformed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in TotalCharges: 22\n"
     ]
    }
   ],
   "source": [
    "nan_count_totalc = get_nan_count(cust_acc_df, 'TotalCharges')\n",
    "\n",
    "print(\"No. of NaN values in TotalCharges:\", nan_count_totalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = cust_acc_df.withColumn('logTotalCharges', log10(col('TotalCharges')))\n",
    "mean_logtotalc = get_col_mean(cust_acc_df, 'logTotalCharges')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'logTotalCharges', mean_logtotalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in logTotalCharges: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count_logtotalc = get_nan_count(cust_acc_df, 'logTotalCharges')\n",
    "print(\"No. of NaN values in logTotalCharges:\", nan_count_logtotalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Account categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|      CONTRACT|CONTRACT_COUNT|\n",
      "+--------------+--------------+\n",
      "|Month-to-month|         19693|\n",
      "|      One year|          4890|\n",
      "|      Two year|          9823|\n",
      "|           NaN|             7|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT CONTRACT, COUNT(*) CONTRACT_COUNT FROM ACC_DF GROUP BY CONTRACT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'Contract', 'Month-to-month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_paperlessbilling = get_col_mode(cust_acc_df, 'PaperlessBilling')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'PaperlessBilling', mode_paperlessbilling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_paymentmethod = get_col_mode(cust_acc_df, 'PaymentMethod')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'PaymentMethod', mode_paymentmethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Demographic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_col in cust_demo_df.columns:\n",
    "    mode_val = get_col_mode(cust_demo_df, demo_col)\n",
    "    cust_demo_df = replace_nan_in_col(cust_demo_df, demo_col, mode_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Services features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serv_col in cust_serv_df.columns:\n",
    "    mode_val = get_col_mode(cust_serv_df, serv_col)\n",
    "    cust_serv_df = replace_nan_in_col(cust_serv_df, serv_col, mode_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OrdinalEncoding for InternetService and Contract\n",
    "These two have an inherent ordering among their categories.\n",
    "\n",
    "**Contract**: Month-to-month -> One year -> two year<br>\n",
    "**InternetService**: No -> DSL -> Fiber Optic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+\n",
      "|InternetService|INTERNET_SERV_COUNT|\n",
      "+---------------+-------------------+\n",
      "|    Fiber optic|              14181|\n",
      "|             No|               5226|\n",
      "|            DSL|              15006|\n",
      "+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT InternetService, COUNT(*) INTERNET_SERV_COUNT FROM SERV_DF GROUP BY InternetService').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "internet_service_categories = ['No', 'DSL', 'Fiber optic']\n",
    "internet_service_order = [1, 2, 3]\n",
    "cust_serv_df = cust_serv_df.withColumn('InternetServiceOrdinal', when(col('InternetService')=='No', 1)\\\n",
    "                                            .otherwise(\n",
    "                                                when(col('InternetService')=='DSL', 2)\\\n",
    "                                                .otherwise(\n",
    "                                                    when(col('InternetService')=='Fiber optic', 3)\\\n",
    "                                                    .otherwise(-1)\n",
    "                                                )\n",
    "                                            )\n",
    "                      )\n",
    "\n",
    "cust_acc_df = cust_acc_df.withColumn('Contract', when(col('Contract')=='Month-to-month', 1)\\\n",
    "                                        .otherwise(\n",
    "                                            when(col('Contract')=='One year', 2)\\\n",
    "                                            .otherwise(\n",
    "                                                when(col('Contract')=='Two year', 3)\\\n",
    "                                                .otherwise(-1)\n",
    "                                            )\n",
    "                                        )\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Tenure and MonthlyCharges\n",
    "As shown in task1.1 the distribution of Tenure differs a lot for churned and non-churned customers for their respective high and low values, so it can be beneficial to create bins for them.<br>\n",
    "<br>\n",
    "For **Tenure**: We can use equal width binning with a bin size of 20 months.<br>\n",
    "For **MonthlyCharges**: We can again use equal width binning with a bin size of 40 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_bucketizer = Bucketizer(splits=[-np.float(\"inf\"), 20, 40, 60, np.float(\"inf\")],\n",
    "                              inputCol='Tenure', outputCol='TenureBuckets', handleInvalid='keep')\n",
    "cust_acc_df = tenure_bucketizer.transform(cust_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthlyc_bucketizer = Bucketizer(splits=[-np.float(\"inf\"), 40, 80, np.float(\"inf\")],\n",
    "                                inputCol='MonthlyCharges', outputCol='MonthlyChargesBuckets', \n",
    "                                handleInvalid='keep')\n",
    "cust_acc_df = monthlyc_bucketizer.transform(cust_acc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Tenure and MonthlyCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_tenure = get_col_mean(cust_acc_df, 'Tenure')\n",
    "std_tenure = cust_acc_df.agg({'Tenure': 'std'}).collect()[0][0]\n",
    "cust_acc_df = cust_acc_df.withColumn('TenureScaled', \n",
    "                                     (cust_acc_df['Tenure']-lit(mean_tenure))/lit(std_tenure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_monthlyc = get_col_mean(cust_acc_df, 'MonthlyCharges')\n",
    "std_monthlyc = cust_acc_df.agg({'MonthlyCharges': 'std'}).collect()[0][0]\n",
    "cust_acc_df = cust_acc_df.withColumn(\"MonthlyChargesScaled\", \n",
    "                                    (cust_acc_df['MonthlyCharges']-lit(mean_monthlyc))/lit(std_monthlyc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing No internet services with No in services dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|       ONLINEBACKUP|count(1)|\n",
      "+-------------------+--------+\n",
      "|                 No|   14064|\n",
      "|                Yes|   15080|\n",
      "|No internet service|    5269|\n",
      "+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT ONLINEBACKUP, COUNT(*) FROM SERV_DF GROUP BY ONLINEBACKUP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_serv_df = cust_serv_df.replace('No internet service', 'No')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate for Tenure and MonthlyCharges\n",
    "Tenure*MonthlyCharges<br>\n",
    "Tenure/MonthlyCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = cust_acc_df.withColumn('Prod_Tenure_MonthlyCharges', \n",
    "                                     cust_acc_df['Tenure']*cust_acc_df['MonthlyCharges'])\n",
    "cust_acc_df = cust_acc_df.withColumn('Div_Tenure_MonthlyCharges',\n",
    "                                    cust_acc_df['Tenure']/cust_acc_df['MonthlyCharges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate of Streaming Services\n",
    "streamingTV_StreamingMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_serv_df = cust_serv_df.withColumn('TV_Movies', \n",
    "                        F.concat(cust_serv_df['StreamingTV'], lit('_'), cust_serv_df['StreamingMovies']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate of Security related features\n",
    "OnlineSecurity_DeviceProtection_TechSupport_OnlineBackup<br>\n",
    "From task1.1 we observed more the services a customer enrolled with greater is the retaining chances, so we can encode this by ordinally encoding the aggregate based on no. of yes values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_serv_df = cust_serv_df.withColumn('Security_Agg',\n",
    "                                      F.concat(\n",
    "                                              col('OnlineSecurity'), lit('_'),\\\n",
    "                                              col('DeviceProtection'), lit('_'),\\\n",
    "                                              col('TechSupport'), lit('_'),\\\n",
    "                                              col('OnlineBackup'), lit('_')\n",
    "                                              )\n",
    "                                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_serv_columns = ['OnlineSecurity', 'OnlineBackup', 'TechSupport', 'DeviceProtection', 'StreamingTV', 'StreamingMovies']\n",
    "for serv_col in numeric_serv_columns:\n",
    "    new_serv_col = serv_col + 'Numeric'\n",
    "    cust_serv_df = cust_serv_df.withColumn(new_serv_col, when(col(serv_col)=='Yes', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_serv_df = cust_serv_df.withColumn('ServicesCount', \n",
    "                        sum([cust_serv_df[serv_col + 'Numeric'] \\\n",
    "                             for serv_col in numeric_serv_columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = cust_acc_df.withColumn('PaperlessBillingNumeric', \n",
    "                                     when(col('PaperlessBilling')=='Yes', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = StringIndexer(inputCol='PaymentMethod', outputCol='PaymentMethodNumeric')\n",
    "si_model = si.fit(cust_acc_df)\n",
    "cust_acc_df = si_model.transform(cust_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mailed check',\n",
       " 'Electronic check',\n",
       " 'Bank transfer (automatic)',\n",
       " 'Credit card (automatic)']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_si = StringIndexer(inputCol='Gender', outputCol='Gender'+'Numeric')\n",
    "demo_si_model = demo_si.fit(cust_demo_df)\n",
    "cust_demo_df = demo_si_model.transform(cust_demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Male', 'Female']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_si_model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_demo_df = cust_demo_df.withColumn('PartnerNumeric', when(col('Partner')=='Yes', 1).otherwise(0))\n",
    "cust_demo_df = cust_demo_df.withColumn('DependentsNumeric', when(col('Dependents')=='Yes', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_serv_df = cust_serv_df.withColumn('PhoneServiceNumeric', when(col('PhoneService')=='Yes', 1).otherwise(0))\n",
    "cust_serv_df = cust_serv_df.withColumn('MultipleLines', when(col('MultipleLines')=='No phone service', 'No'))\n",
    "cust_serv_df = cust_serv_df.withColumn('MultipleLinesNumeric', when(col('MultipleLines')=='Yes', 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(dropLast=False, inputCol='PaymentMethodNumeric', outputCol='PaymentMethodVector')\n",
    "cust_acc_df = ohe.transform(cust_acc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
