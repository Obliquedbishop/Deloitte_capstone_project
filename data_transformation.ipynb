{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import isnull, count, log10, col\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Spark Session and Context objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('ckpt2_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data into Spark Session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_account_info.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_Churn.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_demographics.csv',\n",
       " '/home/hduser/Deloitte_capstone_project/Output_telecomData/WithNaNs/Customer_services.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data_dir_path = os.path.abspath('Output_telecomData')\n",
    "subdirectory_name = 'WithNaNs'\n",
    "\n",
    "data_dir_path = os.path.join(main_data_dir_path, subdirectory_name)\n",
    "assert os.path.exists(data_dir_path)\n",
    "\n",
    "datafile_names = os.listdir(data_dir_path)\n",
    "datafile_paths = [os.path.join(data_dir_path, datafile) for datafile in datafile_names]\n",
    "\n",
    "datafile_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = spark.read.load(datafile_paths[0], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_churn_df = spark.read.load(datafile_paths[1], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_demo_df = spark.read.load(datafile_paths[2], format='csv', sep=',', inferSchema=True, header=True)\n",
    "cust_serv_df = spark.read.load(datafile_paths[3], format='csv', sep=',', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Tenure: double (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_acc_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_churn_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- SeniorCitizen: double (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cust_serv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34413"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_churn_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df.createOrReplaceTempView('acc_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaN Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nan_count(df, col_name):\n",
    "    return df.filter(df[col_name]==np.nan).count()\n",
    "\n",
    "def replace_nan_in_col(df, col_name, by_value=0):\n",
    "    dtypes_dict = dict(df.dtypes)\n",
    "    col_dtype = dtypes_dict[col_name]\n",
    "    if col_dtype!='string':\n",
    "        return df.replace(np.nan, by_value, col_name)\n",
    "    return df.replace('NaN', by_value, col_name)\n",
    "\n",
    "def get_col_mean(df, col_name):\n",
    "    temp = df.replace(np.nan, 0)\n",
    "    return temp.agg({col_name:'avg'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Tenure and Monthly Charges features:\n",
    "As shown in **task1.1** tenure and monthly charges both show normal-like distribution with piled up values at their respective highs and lows. \n",
    "\n",
    "<h4> Basic Treatment Strategy </h4> Mean imputation will work fine as the data is mostly normal like.<br>\n",
    "<h4> Complex Treatment Strategy </h4> Can categorize the data in three parts, the lows, the highs and rest. We can use models like random forest to classify the NaNs of both of the features into these three categories, and finally after classification we can impute the the highs and lows with their respective values and the rests with mean. (<b>But I feel it's a bit overkill as NaN count is not that high</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in Tenure: 16\n",
      "No. of NaN values in MonthlyCharges: 13\n"
     ]
    }
   ],
   "source": [
    "nan_count_tenure = get_nan_count(cust_acc_df, 'Tenure')\n",
    "nan_count_monthlyc = get_nan_count(cust_acc_df, 'MonthlyCharges')\n",
    "\n",
    "print(\"No. of NaN values in Tenure:\", nan_count_tenure)\n",
    "print(\"No. of NaN values in MonthlyCharges:\", nan_count_monthlyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for basic strategy imputation\n",
    "\n",
    "mean_tenure = get_col_mean(cust_acc_df, 'Tenure')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'Tenure', mean_tenure)\n",
    "\n",
    "mean_monthlyc = get_col_mean(cust_acc_df, 'MonthlyCharges')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'MonthlyCharges', mean_monthlyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Total Charges feature:\n",
    "As TotalCharges have a exponentially decreasing distribution, we can perform log transform on it to normalize the distribution.\n",
    "\n",
    "<h4> Imputation Strategy: </h4> Replace with mean of transformed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in TotalCharges: 22\n"
     ]
    }
   ],
   "source": [
    "nan_count_totalc = get_nan_count(cust_acc_df, 'TotalCharges')\n",
    "\n",
    "print(\"No. of NaN values in TotalCharges:\", nan_count_totalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = cust_acc_df.withColumn('logTotalCharges', log10(col('TotalCharges')))\n",
    "mean_logtotalc = get_col_mean(cust_acc_df, 'logTotalCharges')\n",
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'logTotalCharges', mean_logtotalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of NaN values in logTotalCharges: 0\n"
     ]
    }
   ],
   "source": [
    "nan_count_logtotalc = get_nan_count(cust_acc_df, 'logTotalCharges')\n",
    "print(\"No. of NaN values in logTotalCharges:\", nan_count_logtotalc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Contract feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|      CONTRACT|CONTRACT_COUNT|\n",
      "+--------------+--------------+\n",
      "|Month-to-month|         19693|\n",
      "|      One year|          4890|\n",
      "|      Two year|          9823|\n",
      "|           NaN|             7|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT CONTRACT, COUNT(*) CONTRACT_COUNT FROM ACC_DF GROUP BY CONTRACT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_acc_df = replace_nan_in_col(cust_acc_df, 'Contract', 'Month-to-month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
